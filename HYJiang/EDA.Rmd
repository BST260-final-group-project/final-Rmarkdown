---
title: "EDA"
author: "HYJiang"
date: "11/28/2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#### Area vs Price

```{r, echo=FALSE, message=FALSE}
library(ggplot2)
library(dplyr)
data_train <- read.csv("train.csv")
data_train %>% ggplot(aes(x = GrLivArea, y = SalePrice)) +
  geom_point() +
  xlab("Above Ground Living Area") +
  ylab("Sale Price")
```

There are two outliers which has high areas but low sale price. When fitting the models, we delete these two outliers in the training data.

#### Overall Quality vs Price

```{r, message=FALSE, echo=FALSE}
data_train %>% ggplot(aes(x=as.factor(OverallQual), y=SalePrice)) +
  geom_boxplot() +
  xlab("Overall Quality") +
  ylab("Sale Price")
```

Conincided with our intuition, if the overall quality of the house is better, then the house price is higher.

#### Year vs Price
```{r, echo=FALSE}
data_train %>% ggplot(aes(x=as.factor(YearBuilt), y=SalePrice)) +
  geom_boxplot() +
  xlab("Year Built") +
  ylab("Sale Price") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

```{r, echo=FALSE}
data_train %>% ggplot(aes(x=as.factor(YearRemodAdd), y=SalePrice)) +
  geom_boxplot() +
  xlab("Year Remodel") +
  ylab("Sale Price") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

In general, the newer the house is, the higher the price is. But the correlation is not very strong.


#### Lasso

The result of lasso regression on non-scale data is as follows. We also show the cofficients of lasso regression under the optimal parameter.

```{r, echo=FALSE, message=FALSE}
# Lasso library
library(assertive, warn.conflicts = FALSE)
library(glmnet)
library(dplyr)

# Set seed
set.seed(260)
data_all <- read.csv("feature_fillna_onehot.csv")
data_all <- data_all %>% mutate(TotalSF = TotalBsmtSF + X1stFlrSF + X2ndFlrSF)

# Delete "Id" and "source" in train data
data_train <- data_all %>% filter(source == "train")
data_train <- subset(data_train, select = -Id)
data_train <- subset(data_train, select = -source)

# X and y for training
y_train <- data_train$SalePrice
X_train <- subset(data_train, select = -SalePrice)

# Scale numerical features
#col_num <- c("LotFrontage", "LotArea", "MasVnrArea", "BsmtFinSF1", "BsmtFinSF2", "BsmtUnfSF", 
#               "TotalBsmtSF", "X1stFlrSF", "X2ndFlrSF", "LowQualFinSF", "GrLivArea", "TotRmsAbvGrd", 
#               "GarageArea", "WoodDeckSF", "OpenPorchSF", "EnclosedPorch", "X3SsnPorch", "ScreenPorch",
#              "PoolArea", "MiscVal", "TotalSF")
#for (col in col_num){
#  X_train[col] <- scale(X_train[col])
#}

# It is easier to interpret the result if the data is not scaled
# Lasso regression
fit = glmnet(as.matrix(X_train), y_train)
plot(fit)

# Cross validation to find the optimal lambda
cvfit <- cv.glmnet(as.matrix(X_train), y_train)
coef(cvfit, s = "lambda.min")
```


Based on the result of lasso regression, we can know the average price for some house components. A fire place is worth 3000 dollars while a full bath only costs us 130 dollars (the price of bathroom is a little weired). The coefficient of RoofMatl_ClyTile is -527,000, which means that house would be super cheap if its roof material is clay tile. The lasso also tells us that it doesn't matter in which month you sell your house.
